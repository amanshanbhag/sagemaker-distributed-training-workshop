{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5176b1cd-324b-4b0e-a645-87cfbd795d34",
   "metadata": {},
   "source": [
    "# Pre-train Llama 3 models with torchtitan on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddf2b4-9a21-4f73-a262-e83ec85d14f2",
   "metadata": {},
   "source": [
    "[torchtitan](https://github.com/pytorch/torchtitan/tree/main) is a reference architecture for large-scale LLM training using native PyTorch. It aims to showcase PyTorchâ€™s latest distributed training features in a clean, minimal code base. The library is designed to be simple to understand, use, and extend for different training purposes, with minimal changes required to the model code when applying various parallel processing techniques.\n",
    "\n",
    "\n",
    "In this notebook, we showcase how the torchtitan library accelerates and simplifies the pre-training of Meta Llama 3-like model architectures. We showcase the key features and capabilities of torchtitan such as FSDP2, torch.compile integration, and FP8 support that optimize the training efficiency. We pre-train a Meta Llama 3 8B model architecture using torchtitan on Amazon SageMaker on p5.48xlarge instances, each equipped with 8 Nvidia H100 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997becc-4ea9-4020-90d6-e164b0dd4a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2506727-243f-455f-908d-795a8af4a764",
   "metadata": {
    "tags": []
   },
   "source": [
    "You need to run the Notebook from [**Step 1-Build your Custom Container Jupyter Notebook**](https://github.com/aws-samples/sagemaker-distributed-training-workshop/blob/main/14_torchtitan/Step%201%20-Build%20Custom%20Container.ipynb) to build the torchtitan custom container for training your model. Optionally,  to use your custom dataset, you can follow the instructions in the [**Step 2: Prepare your Dataset Jupyter Notebook**](https://github.com/aws-samples/sagemaker-distributed-training-workshop/blob/main/14_torchtitan/(Optional)%20Step%202%20-Prepare%20Dataset.ipynb) where we showcase how to download a sample dataset(s) to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b556b-3182-411d-bb6d-b80eafc0d9bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Amazon SageMaker Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9260e29-8aaf-4834-8f60-37deeb76f093",
   "metadata": {},
   "source": [
    "Upgrade SageMaker SDK to the latest version.\n",
    "NOTE: This step might require a kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1da696-76b6-410e-9448-157a497997a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb9a71-d313-4e36-bc0d-be2039e6d2f8",
   "metadata": {},
   "source": [
    "Run the following cells to import SageMaker modules and retrieve information of your current SageMaker environment, such as your AWS account ID, the AWS Region, and the ARN of your Amazon SageMaker Execution Role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4096f5-00a4-4f89-8164-757a2974c915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print(\"Default bucket for this session: \", default_bucket)\n",
    "\n",
    "#set default path for data channels\n",
    "data_channels=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f91b8-ccb4-46b0-a9bf-8616266c9394",
   "metadata": {},
   "source": [
    "### Clone the torchtitan repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86d23b-c5c8-48b2-81b5-30bf7b0a94f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/torchtitan.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f09c9-268c-4e3b-bf2f-cf55400bcbf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we create a source directory that will contain the the training source code and dependencies required to execute the training. We also move the required dependencies from the torchtitan directory to the source directory. You can refer to the documentation to learn about the [Default Paths for Training Storage Locations](https://docs.aws.amazon.com/sagemaker/latest/dg/model-train-storage.html#model-train-storage-env-var-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99de369-aa5d-453c-8cc1-2788176b7a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir torchtitan/src\n",
    "!mv  torchtitan/torchtitan/ torchtitan/train_configs/ torchtitan/train.py  torchtitan/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17a6bc-d76c-4d74-a87e-6b3db8645314",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Downloading a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac21edd-1454-4462-8621-aba80abb9302",
   "metadata": {},
   "source": [
    "We will need the Llama-3 tokenizer that will be used to pre-process the dataset to generate tokens. Provide your Hugging Face token in the command below, for **--hf_token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8ee33-8d6c-452c-8c01-ffe1048d4f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir torchtitan/src/llama-3-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bcd5b6-b896-4b53-8be3-d7daa15d2b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python torchtitan/src/torchtitan/datasets/download_tokenizer.py --repo_id meta-llama/Meta-Llama-3-8B --local_dir torchtitan/src/llama-3-tokenizer  --tokenizer_path \"original\" --hf_token=...--hf_token=\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50913c56-a14d-4e57-8a8b-2ff6bb997561",
   "metadata": {},
   "source": [
    "### Update the LLama-3-8b toml configuration file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfd87f-37d9-418f-9a3d-49ebc3867014",
   "metadata": {
    "tags": []
   },
   "source": [
    "The options for training models with torchtitan are easily configured via the TOML files. In this tutorial,  we will be working with the Llama3 8B TOML file located in torchtitan/src/train_configs/ to configure the training options. We will need to modify the sections below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43df3c1-4739-413d-9431-8cc149a566d5",
   "metadata": {},
   "source": [
    "1. Enable Tensorboard profiling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ef257-0a01-4715-9327-d04d4f0b2e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[metrics]\n",
    "log_freq = 10\n",
    "enable_tensorboard = true\n",
    "save_tb_folder = \"/opt/ml/output/tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965418f-3caa-4398-85e6-81bbc35b5adf",
   "metadata": {},
   "source": [
    "**2.Enable torch.compile.**  torch.compile is a key feature in PyTorch that significantly boosts model performance with minimal code changes. Through its just-in-time (JIT) compilation, it analyzes and transforms PyTorch code into more efficient kernels. TorchTitan supports torch.compile, which delivers substantial speedups, especially for large models and complex architectures, by leveraging techniques like operator fusion, memory planning, and automatic kernel selection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040243f2-cfca-4549-814f-314326a2c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile = true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d4b606-b73d-4dad-b88a-426494c39794",
   "metadata": {},
   "source": [
    "**3. Enable FP8 linear operations**  torchtitan provides support for FP8 (8-bit floating point) computation that  reduces memory footprint and enhances performance in large language model training. FP8 has two formats, E4M3 and E5M2, each optimized for different aspects of training. E4M3 offers higher precision, making it ideal for forward propagation, while E5M2, with its larger dynamic range, is better suited for backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da731c95-644e-45a0-93e1-96f259bdd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_float8_linear = true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f79ae2-0d02-4ae9-b8c8-471b748294e4",
   "metadata": {},
   "source": [
    "**4. Enable FP8 all-gather.**  This feature enables efficient communication of FP8 tensors across multiple GPUs, significantly reducing network bandwidth compared to bfloat16 all-gather operations. FP8 all-gather performs float8 casting before the all-gather operation, reducing the message size. Key to its efficiency is the combined AMAX( absolute maximum) AllReduce, which calculates AMAX for all float8 parameters in a single operation after the optimizer step, avoiding multiple small all-reduces. Similar to FP8 support, this also has no impact on model accuracy, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce812ec3-e7a4-4a02-9765-2e00c1e58a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_fsdp_float8_all_gather= true\n",
    "precompute_float8_dynamic_scale_for_fsdp = true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179af7c7-7c84-4711-8123-cde493da5372",
   "metadata": {},
   "source": [
    "Below is the full updated configuration with the above optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab822a3-d6fc-49ae-bcc4-ee6cd4a666a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile torchtitan/src/train_configs/llama3_8b_optimisations.toml\n",
    "# torchtitan Config.toml\n",
    "\n",
    "[job]\n",
    "dump_folder = \"./outputs\"\n",
    "description = \"Llama 3 8B training\"\n",
    "\n",
    "[profiling]\n",
    "enable_profiling = false\n",
    "save_traces_folder = \"profile_trace\"\n",
    "profile_freq = 100\n",
    "\n",
    "[metrics]\n",
    "log_freq = 10\n",
    "enable_tensorboard = true\n",
    "save_tb_folder = \"/opt/ml/output/tensorboard\"\n",
    "\n",
    "[model]\n",
    "name = \"llama3\"\n",
    "flavor = \"8B\"\n",
    "norm_type = \"rmsnorm\"  # layernorm / np_layernorm / rmsnorm / fused_rmsnorm\n",
    "tokenizer_path = \"./llama-3-tokenizer/original/tokenizer.model\"\n",
    "\n",
    "[optimizer]\n",
    "name = \"AdamW\"\n",
    "lr = 3e-4\n",
    "\n",
    "[training]\n",
    "batch_size = 1\n",
    "seq_len = 8192\n",
    "warmup_steps = 200  # lr scheduler warm up\n",
    "max_norm = 1.0  # grad norm clipping\n",
    "steps = 1000\n",
    "data_parallel_degree = -1\n",
    "tensor_parallel_degree = 1\n",
    "compile = true\n",
    "dataset = \"c4\"\n",
    "\n",
    "[experimental]\n",
    "pipeline_parallel_degree = 1\n",
    "\n",
    "[checkpoint]\n",
    "enable_checkpoint = false\n",
    "folder = \"checkpoint\"\n",
    "interval_type = \"steps\"\n",
    "interval = 500\n",
    "model_weights_only = false\n",
    "export_dtype = \"float32\"\n",
    "async_mode = \"disabled\" # [\"disabled\", \"async\", \"async_with_pinned_mem\"]\n",
    "\n",
    "[activation_checkpoint]\n",
    "mode = 'selective'  # ['none', 'selective', 'full']\n",
    "selective_ac_option = 'op'  # 'int' = ac every positive int layer or 'op', ac based on ops policy\n",
    "\n",
    "[float8]\n",
    "enable_float8_linear = true\n",
    "enable_fsdp_float8_all_gather= true\n",
    "precompute_float8_dynamic_scale_for_fsdp = true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ffa48-5182-4eb5-a42a-d58f2ef5d5c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure Tensorboard for estimator function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9c16b-60bd-4b1d-9dae-de0a7fa814c9",
   "metadata": {},
   "source": [
    "Next, to monitor our training progress, we'll set up TensorBoard output. This will allow us to visualize our training metrics in real-time, providing valuable insights into how our model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17b010-58a2-4315-9c1f-c4b60bf6361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "LOG_DIR=\"/opt/ml/output/tensorboard\"\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=f\"s3://sagemaker-{region}-{account}/tensorboard/\",\n",
    "    container_local_output_path=LOG_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20000f2b-f131-4976-aa9d-da7d5243651e",
   "metadata": {},
   "source": [
    "### Create the SageMaker estimator function for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c03ac4-1a16-4098-b467-0e5a4c9a7199",
   "metadata": {},
   "source": [
    "Before launching the training, we need to modify the torchtitan/src/train.py file to be able to parse the TOML configuration file as a hyperparameter  in our training estimator function. Update the main function in torchtitan/src/train.py to below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b39fb-da97-4379-95f1-25fd286d2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import argparse\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config_file\", type=str, default=\"\", help=\"Model config file\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    # Load the configuration from the downloaded file\n",
    "    config = JobConfig()\n",
    "    config.parse_args([\"--job.config_file\", args.config_file])\n",
    "    main(config)\n",
    "    \n",
    "    torch.distributed.destroy_process_group()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87ce56-9f82-4966-aebb-68cfc5d8a517",
   "metadata": {},
   "source": [
    "Next, we need to update the **image_uri** in the estimator function to point to the  custom training image generated from [**Step 1-Build your Custom Container Jupyter Notebook**](https://github.com/aws-samples/sagemaker-distributed-training-workshop/blob/main/14_torchtitan/Step%201%20-Build%20Custom%20Container.ipynb) We also provide the path to the TOML configuration file generated above as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1122252-d0e8-43cf-93df-fbf8b07a0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "hyperparameters = {\n",
    "    \"config_file\": \"train_configs/llama3_8b_optimisations.toml\"\n",
    "}\n",
    "env = {}\n",
    "env['HF_HUB_ETAG_TIMEOUT'] = '500'\n",
    "\n",
    "timestamp = strftime(\"%Y-%m-%d-%H-%M\", gmtime())\n",
    "\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    base_job_name=f'llama3-8b-compile-fp8-fp8-comms-{timestamp}',\n",
    "    entry_point=\"train.py\",\n",
    "    image_uri=\"<provide-path-to-image-uri>\",\n",
    "    source_dir=os.path.join(os.getcwd(), \"torchtitan/src\"),\n",
    "    role=role,\n",
    "    instance_type=\"ml.p5.48xlarge\",\n",
    "    volume_size=800,\n",
    "    instance_count=4,\n",
    "    environment=env,\n",
    "    hyperparameters=hyperparameters,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    distribution={\n",
    "    'torch_distributed': {'enabled': True},\n",
    "    },\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044e4ef-39f8-4427-84d0-fcacb039a698",
   "metadata": {},
   "source": [
    "Then we finally, launch the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e717ff0-3683-44d0-bf6e-d4738b5116f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_estimator.fit(inputs=data_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed4d83-6db6-4ec5-8179-e10cc86672bd",
   "metadata": {},
   "source": [
    "## (Optional) Training with your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b927ff-bc20-4b57-a55b-9a460ef5e31c",
   "metadata": {},
   "source": [
    "In the previous training run, we used the  allenai/c4 which is the default dataset pre-configured for the torchtitan samples and is streamed directly from Hugging Face hub during training. However, if you have your  dataset residing in S3, you need to configure the input data channels below to point to your dataset. We have provided a sample in the  [**Step 2: Prepare your Dataset Jupyter Notebook**](https://github.com/aws-samples/sagemaker-distributed-training-workshop/blob/main/14_torchtitan/(Optional)%20Step%202%20-Prepare%20Dataset.ipynb)  showcasing how you can download the  the allenai/c4 dataset to S3 to simulate a dataset residing in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2db96-a044-4d6d-addb-b010d70c2889",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we need to add the utility function below to the torchtitan/src/torchtitan/datasets/hf_datasets.py to load our dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6596d995-e9f6-4e97-b92f-915f7537b71a",
   "metadata": {},
   "source": [
    "def load_parquet_dataset(dataset_path):\n",
    "    parquet_files = [f for f in os.listdir(dataset_path) if f.endswith(\".parquet\")]\n",
    "    datasets = []\n",
    "\n",
    "    for parquet_file in parquet_files:\n",
    "        file_path = os.path.join(dataset_path, parquet_file)\n",
    "        table = pq.read_table(file_path)\n",
    "        dataset = Dataset(table)\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    if len(datasets) == 0:\n",
    "        raise ValueError(\"No valid Parquet files found in the dataset directory.\")\n",
    "\n",
    "    return datasets[0] if len(datasets) == 1 else concatenate_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0160849-782d-48c8-8cb8-38a2a3399abe",
   "metadata": {},
   "source": [
    "We also need to add our custom dataset to the supported datasets configuration in torchtitan/src/torchtitan/datasets/hf_datasets.py and provide the path to \"/opt/ml/input/data/train/\" where the data channel directory is written to."
   ]
  },
  {
   "cell_type": "raw",
   "id": "095b27a0-fddc-4ca9-9a44-66fd5d459809",
   "metadata": {},
   "source": [
    "_supported_datasets = {\n",
    "    \"c4_mini\": \"torchtitan/datasets/c4_mini\",\n",
    "    \"c4\": \"allenai/c4\",\n",
    "    \"c4_custom\": \"/opt/ml/input/data/train/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94882f5-4c7f-43d5-b4ac-ef9d653570fa",
   "metadata": {},
   "source": [
    "Finally, we can add a condition to handle our dataset in torchtitan/src/torchtitan/datasets/hf_datasets.py in the \"def __init__()\" function in the \"class HuggingFaceDataset(IterableDataset, Stateful):\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "8af2928f-9cd3-46e7-bb4b-a12d241c0900",
   "metadata": {},
   "source": [
    "if dataset_name == \"c4_custom\":\n",
    "            logger.info(f\"Loading dataset from {dataset_path}\")\n",
    "\n",
    "            try:\n",
    "                ds = load_arrow_dataset(dataset_path)\n",
    "                logger.info(f\"Loaded dataset with {len(ds)} examples\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading dataset: {str(e)}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32820b06-9cfe-40fc-b384-9d1bdc4ac0ac",
   "metadata": {},
   "source": [
    "**Important** Remember to upload the dataset entry section in your TOML configuration file to point to the name of your custom dataset e.g for the example, we will set this to dataset=\"c4_custom\" to correspond to the above steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a608433-04b9-42fe-9e5c-c60420a74cfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we set up the data channels for SageMaker training by creating TrainingInput objects from the provided S3 bucket paths for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1076b-723b-4d97-b9b7-fa32a620c597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dataset_location = \"<path-to-S3-dataset>\"\n",
    "\n",
    "s3_train_bucket = training_dataset_location\n",
    "\n",
    "if s3_train_bucket != None:\n",
    "    train = sagemaker.inputs.TrainingInput(s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\")\n",
    "    data_channels = {\"train\": train}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b987d12-7080-4440-a344-2a9d280aaa9c",
   "metadata": {},
   "source": [
    "We can then launch the training as shown with the estimator function in the previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b693c-4cdf-4060-8889-b777d23a84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "hyperparameters = {\n",
    "    \"config_file\": \"train_configs/llama3_8b_optimisations.toml\"\n",
    "}\n",
    "env = {}\n",
    "env['HF_HUB_ETAG_TIMEOUT'] = '500'\n",
    "\n",
    "timestamp = strftime(\"%Y-%m-%d-%H-%M\", gmtime())\n",
    "\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    base_job_name=f'llama3-8b-compile-fp8-fp8-comms-{timestamp}',\n",
    "    entry_point=\"train.py\",\n",
    "    image_uri=\"<provide-path-to-image-uri>\",\n",
    "    source_dir=os.path.join(os.getcwd(), \"torchtitan/src\"),\n",
    "    role=role,\n",
    "    instance_type=\"ml.p5.48xlarge\",\n",
    "    volume_size=800,\n",
    "    instance_count=4,\n",
    "    environment=env,\n",
    "    hyperparameters=hyperparameters,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    distribution={\n",
    "    'torch_distributed': {'enabled': True},\n",
    "    },\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86785f7f-59ae-4def-bd2e-10882c5527f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fefc80-606a-46bd-bf7d-073ad18b9abf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Perfomance Comparison with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384758a-151c-4112-b8c1-aebef15922c3",
   "metadata": {},
   "source": [
    "To effectively evaluate the performance speedup from the optimization techniques, consider the following approach:\n",
    "\n",
    "- Create a baseline training job without the optimizations. \n",
    "- Run subsequent jobs, adding each optimization step wise - starting with torch.compile, then FP8, and lastly FP8 all-gather.\n",
    "- Compare the throughput(tokens per second) of each job to assess the impact of the optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536afb6b-4e35-478a-b0c7-3e02b6d3990e",
   "metadata": {},
   "source": [
    "You can then visualize the results on [Tensorboard](https://docs.aws.amazon.com/sagemaker/latest/dg/tensorboard-on-sagemaker.html) to compare the performance and corresponding loss curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ace9dc-777e-443f-bfaf-d836d193e084",
   "metadata": {},
   "source": [
    "Package versions used in this tutorial:\n",
    "torchtitan hash commit : ac90c36e39c6274f9beaf76922627665b6553905\n",
    "torch==2.5.0.dev20240906+cu121\n",
    "torchao==0.6.0.dev20240907+cu121"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
