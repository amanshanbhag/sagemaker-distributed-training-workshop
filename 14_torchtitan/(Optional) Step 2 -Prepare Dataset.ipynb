{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557d9ff5-b8cb-4180-811e-aa2e23b788ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (Optional) Step 2: Prepare your Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01fec4-a6ec-4675-8c1f-d5e5d319fb2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "By default, the torchtitan library uses the allenai/c4 dataset in its training configuration. This is streamed directly during training. \n",
    "\n",
    "However, you may want to pre-train your models on your own dataset residing in S3. In this notebook, we will download the allenai/c4 dataset to s3 and in the training notebook , we will show you how you can configure the torchtitan library to use the dataset residing in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e52fa-ab8c-49d1-97b0-b6c66e4dcaff",
   "metadata": {},
   "source": [
    "We first create a processing script to download the dataset from HuggingFace in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e40472-ba1e-4c09-837d-f945dbd53679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%%writefile download_c4_dataset.py\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"tqdm\"])\n",
    "\n",
    "\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_and_upload_chunk(args):\n",
    "    output_path, chunk_id, chunk = args\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing chunk {chunk_id}\")\n",
    "        # Convert chunk to Arrow table\n",
    "        arrow_table = pa.Table.from_pylist(chunk)\n",
    "        \n",
    "        # Write the table to an in-memory buffer as Parquet\n",
    "        buf = io.BytesIO()\n",
    "        pq.write_table(arrow_table, buf)\n",
    "        \n",
    "        # Get the bytes from the buffer\n",
    "        parquet_bytes = buf.getvalue()\n",
    "        \n",
    "        # Save to local file\n",
    "        file_path = os.path.join(output_path, f\"chunk_{chunk_id:06d}.parquet\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(parquet_bytes)\n",
    "        \n",
    "        return chunk_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {chunk_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Process C4 dataset')\n",
    "    parser.add_argument('--input-data', type=str, help='S3 path to input data (optional)')\n",
    "    parser.add_argument('--output-dir', type=str, default='/opt/ml/processing/output', help='Output directory')\n",
    "    parser.add_argument('--chunk-size', type=int, default=10000, help='Number of examples per chunk')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    output_path = args.output_dir\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    print(\"Loading C4 dataset...\")\n",
    "    dataset_args = {\"streaming\": True}\n",
    "    if args.input_data:\n",
    "        dataset_args[\"data_files\"] = args.input_data\n",
    "    dataset = load_dataset(\"c4\", \"en\", split=\"train\", **dataset_args)\n",
    "\n",
    "    print(f\"Saving dataset to {output_path}\")\n",
    "\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print(f\"Number of CPUs available: {num_cpus}\")\n",
    "\n",
    "    chunk_size = args.chunk_size\n",
    "    chunk = []\n",
    "    chunk_id = 0\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_cpus) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        with tqdm(total=None) as pbar:\n",
    "            for example in dataset:\n",
    "                chunk.append(example)\n",
    "                if len(chunk) >= chunk_size:\n",
    "                    future_args = (output_path, chunk_id, chunk)\n",
    "                    futures.append(executor.submit(process_and_upload_chunk, future_args))\n",
    "                    chunk = []\n",
    "                    chunk_id += 1\n",
    "                    pbar.update(chunk_size)\n",
    "\n",
    "            if chunk:\n",
    "                future_args = (output_path, chunk_id, chunk)\n",
    "                futures.append(executor.submit(process_and_upload_chunk, future_args))\n",
    "                pbar.update(len(chunk))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                print(f\"Completed processing chunk {result}\")\n",
    "\n",
    "    print(\"Dataset processing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df659c7e-4f88-4575-9cb1-ee49ea1a3311",
   "metadata": {},
   "source": [
    "We then launch the above script using a [SageMaker Processsing Job](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) to download the dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da564db0-122a-489f-a8c4-dac01c45d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "# Setup\n",
    "role = get_execution_role()\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print(\"Default bucket for this session: \", default_bucket)\n",
    "\n",
    "# Create a custom Processor\n",
    "script_processor = ScriptProcessor(\n",
    "    role=role,\n",
    "    image_uri=sagemaker.image_uris.retrieve(\n",
    "        framework=\"sklearn\",\n",
    "        region=region,\n",
    "        version=\"0.23-1\",\n",
    "    ),\n",
    "    instance_count=10,\n",
    "    instance_type=\"ml.c5.9xlarge\",\n",
    "    base_job_name='c4-dataset-processing',\n",
    "    command=[\"python3\"],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# Set up the processing job\n",
    "script_processor.run(\n",
    "    code=\"download_c4_dataset.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"c4_dataset\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"s3://{default_bucket}/c4-dataset\",\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--output-dir\", \"/opt/ml/processing/output\",\n",
    "        \"--chunk-size\", \"100000\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Processing job '{script_processor.latest_job.job_name}' started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff1060-41d6-4c7f-bb53-be68f3122df2",
   "metadata": {},
   "source": [
    "Please note down the S3 path where the dataset is downloaded,  as we will use in the torchtitan training Notebook as the input channel for the training estimator function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
